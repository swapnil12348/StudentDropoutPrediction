import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import lime
import lime.lime_tabular
from sklearn.inspection import permutation_importance
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Page configuration
st.set_page_config(page_title="Student Dropout Prediction", page_icon="üéì", layout="wide")

# Create a mapping dictionary for target labels
TARGET_MAPPING = {
    'Graduate': 1,
    'Dropout': 0,
    'Enrolled': 2
}

def display_data_quality(df):
    """Display data quality assessment with labeled charts and insights"""
    st.markdown("<h2 style='font-size: 20px; font-weight: bold;'>üîç Data Quality Assessment</h2>", unsafe_allow_html=True)
    col1, col2 = st.columns([1, 1])

    with col1:
        st.markdown("<h3 style='font-size: 16px;'>Missing Data by Column</h3>", unsafe_allow_html=True)
        missing_data = df.isnull().sum()
        missing_data = missing_data[missing_data > 0].sort_values(ascending=False)
        if len(missing_data) > 0:
            fig, ax = plt.subplots(figsize=(8, 5))
            bars = missing_data.plot(kind='bar', ax=ax, color='coral')
            ax.set_title('Missing Data Across Columns', fontsize=14, fontweight='bold', pad=15)
            ax.set_ylabel('Number of Missing Values', fontsize=12)
            ax.set_xlabel('Dataset Columns', fontsize=12)
            ax.tick_params(axis='both', labelsize=10)
            plt.xticks(rotation=45, ha='right')
            for bar in bars.patches:
                height = bar.get_height()
                ax.annotate(f'{int(height)}',
                           xy=(bar.get_x() + bar.get_width() / 2, height),
                           xytext=(0, 3), textcoords='offset points',
                           ha='center', va='bottom', fontsize=10)
            plt.tight_layout()
            st.pyplot(fig)
            st.markdown("**Insight**: Columns with high missing values may require data imputation or removal to ensure reliable analysis.", unsafe_allow_html=True)
            plt.close()
        else:
            st.success("‚úÖ No missing data found in the dataset!")
            st.markdown("**Insight**: The dataset is complete, ensuring robust analysis without the need for imputation.", unsafe_allow_html=True)

    with col2:
        st.markdown("<h3 style='font-size: 16px;'>Distribution of Data Types</h3>", unsafe_allow_html=True)
        dtype_counts = df.dtypes.value_counts()
        fig, ax = plt.subplots(figsize=(8, 5))
        colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold']
        labels = [str(dtype) for dtype in dtype_counts.index]
        ax.pie(dtype_counts.values, labels=labels, autopct='%1.1f%%',
               colors=colors[:len(dtype_counts)], textprops={'fontsize': 10})
        ax.set_title('Proportion of Data Types in Dataset', fontsize=14, fontweight='bold', pad=15)
        plt.tight_layout()
        st.pyplot(fig)
        st.markdown("**Insight**: The mix of data types (e.g., numeric, categorical) influences preprocessing needs for modeling.", unsafe_allow_html=True)
        plt.close()

def display_demographics(df):
    """Display student demographics insights with labeled charts and insights"""
    st.markdown("<h2 style='font-size: 20px; font-weight: bold;'>üë• Student Demographics Insights</h2>", unsafe_allow_html=True)
    col1, col2, col3 = st.columns([1, 1, 1])

    with col1:
        st.markdown("<h3 style='font-size: 16px;'>Age Distribution at Enrollment</h3>", unsafe_allow_html=True)
        if 'Age at enrollment' in df.columns:
            fig, ax = plt.subplots(figsize=(8, 5))
            df['Age at enrollment'].hist(bins=20, ax=ax, color='lightblue', alpha=0.7, edgecolor='black')
            mean_age = df['Age at enrollment'].mean()
            ax.axvline(mean_age, color='red', linestyle='--',
                      label=f'Mean Age: {mean_age:.1f}')
            ax.set_title('Distribution of Student Age at Enrollment', fontsize=14, fontweight='bold', pad=15)
            ax.set_xlabel('Age (Years)', fontsize=12)
            ax.set_ylabel('Number of Students', fontsize=12)
            ax.tick_params(axis='both', labelsize=10)
            ax.legend(fontsize=10)
            plt.tight_layout()
            st.pyplot(fig)
            st.markdown(f"**Insight**: Most students enroll around age {int(mean_age)}, with older students potentially facing higher dropout risks.", unsafe_allow_html=True)
            plt.close()

    with col2:
        st.markdown("<h3 style='font-size: 16px;'>Gender Distribution</h3>", unsafe_allow_html=True)
        if 'Gender' in df.columns:
            gender_counts = df['Gender'].value_counts()
            gender_labels = ['Female' if x == 0 else 'Male' for x in gender_counts.index]
            fig, ax = plt.subplots(figsize=(8, 5))
            ax.pie(gender_counts.values, labels=gender_labels, autopct='%1.1f%%',
                  colors=['pink', 'lightblue'], textprops={'fontsize': 10})
            ax.set_title('Gender Distribution of Students', fontsize=14, fontweight='bold', pad=15)
            plt.tight_layout()
            st.pyplot(fig)
            st.markdown("**Insight**: Gender balance affects dropout rates, with potential differences in academic support needs.", unsafe_allow_html=True)
            plt.close()

    with col3:
        st.markdown("<h3 style='font-size: 16px;'>Scholarship Status</h3>", unsafe_allow_html=True)
        if 'Scholarship holder' in df.columns:
            scholarship_counts = df['Scholarship holder'].value_counts()
            scholarship_labels = ['No Scholarship' if x == 0 else 'Scholarship' for x in scholarship_counts.index]
            fig, ax = plt.subplots(figsize=(8, 5))
            ax.pie(scholarship_counts.values, labels=scholarship_labels, autopct='%1.1f%%',
                  colors=['lightcoral', 'lightgreen'], textprops={'fontsize': 10})
            ax.set_title('Proportion of Scholarship Holders', fontsize=14, fontweight='bold', pad=15)
            plt.tight_layout()
            st.pyplot(fig)
            st.markdown("**Insight**: Students without scholarships may face financial stress, increasing dropout likelihood.", unsafe_allow_html=True)
            plt.close()

def display_academic_performance(df):
    """Display academic performance overview with labeled charts and insights"""
    st.markdown("<h2 style='font-size: 20px; font-weight: bold;'>üìö Academic Performance Overview</h2>", unsafe_allow_html=True)
    st.info("üí° **What this shows:** How well students performed academically and how it relates to their final outcome")
    col1, col2 = st.columns([1, 1])

    with col1:
        st.markdown("<h3 style='font-size: 16px;'>Admission Grades by Outcome</h3>", unsafe_allow_html=True)
        if 'Admission grade' in df.columns:
            fig, ax = plt.subplots(figsize=(8, 5))
            colors = {'Graduate': 'green', 'Dropout': 'red', 'Enrolled': 'orange'}
            for outcome in df['Target'].unique():
                subset = df[df['Target'] == outcome]['Admission grade']
                ax.hist(subset, alpha=0.6, label=f'{outcome} ({len(subset)})',
                       bins=15, color=colors.get(outcome, 'gray'))
                mean_val = subset.mean()
                ax.axvline(mean_val, color=colors.get(outcome, 'gray'), linestyle='--',
                          label=f'{outcome} Mean: {mean_val:.1f}')
            ax.set_title('Admission Grades by Student Outcome', fontsize=14, fontweight='bold', pad=15)
            ax.set_xlabel('Admission Grade (0-200)', fontsize=12)
            ax.set_ylabel('Number of Students', fontsize=12)
            ax.legend(fontsize=10)
            ax.grid(True, alpha=0.3)
            ax.tick_params(axis='both', labelsize=10)
            plt.tight_layout()
            st.pyplot(fig)
            st.markdown("**Insight**: Higher admission grades are strongly associated with graduation, while lower grades correlate with dropouts.", unsafe_allow_html=True)
            plt.close()

    with col2:
        st.markdown("<h3 style='font-size: 16px;'>First Semester Grades by Outcome</h3>", unsafe_allow_html=True)
        if 'Curricular units 1st sem (grade)' in df.columns:
            fig, ax = plt.subplots(figsize=(8, 5))
            colors = {'Graduate': 'green', 'Dropout': 'red', 'Enrolled': 'orange'}
            for outcome in df['Target'].unique():
                subset = df[df['Target'] == outcome]['Curricular units 1st sem (grade)']
                ax.hist(subset, alpha=0.6, label=f'{outcome} ({len(subset)})',
                       bins=15, color=colors.get(outcome, 'gray'))
                mean_val = subset.mean()
                ax.axvline(mean_val, color=colors.get(outcome, 'gray'), linestyle='--',
                          label=f'{outcome} Mean: {mean_val:.1f}')
            ax.set_title('First Semester Grades by Student Outcome', fontsize=14, fontweight='bold', pad=15)
            ax.set_xlabel('Average Grade (0-20)', fontsize=12)
            ax.set_ylabel('Number of Students', fontsize=12)
            ax.legend(fontsize=10)
            ax.grid(True, alpha=0.3)
            ax.tick_params(axis='both', labelsize=10)
            plt.tight_layout()
            st.pyplot(fig)
            st.markdown("**Insight**: Strong first-semester performance is a key predictor of graduation success.", unsafe_allow_html=True)
            plt.close()

def display_performance_summary(df):
    """Display performance summary metrics"""
    st.markdown("<h2 style='font-size: 20px; font-weight: bold;'>üìà Performance Summary</h2>", unsafe_allow_html=True)
    if 'Admission grade' in df.columns and 'Curricular units 1st sem (grade)' in df.columns:
        summary_col1, summary_col2, summary_col3 = st.columns(3)
        with summary_col1:
            st.metric(
                label="Average Admission Grade (Graduates)",
                value=f"{df[df['Target'] == 'Graduate']['Admission grade'].mean():.1f}",
                delta=f"{df[df['Target'] == 'Graduate']['Admission grade'].mean() - df[df['Target'] == 'Dropout']['Admission grade'].mean():.1f} vs Dropouts"
            )
        with summary_col2:
            st.metric(
                label="Average 1st Sem Grade (Graduates)",
                value=f"{df[df['Target'] == 'Graduate']['Curricular units 1st sem (grade)'].mean():.1f}",
                delta=f"{df[df['Target'] == 'Graduate']['Curricular units 1st sem (grade)'].mean() - df[df['Target'] == 'Dropout']['Curricular units 1st sem (grade)'].mean():.1f} vs Dropouts"
            )
        with summary_col3:
            high_performers = df[df['Curricular units 1st sem (grade)'] > 15]
            success_rate = (high_performers['Target'] == 'Graduate').mean() * 100 if len(high_performers) > 0 else 0
            st.metric(
                label="Success Rate (Grade >15)",
                value=f"{success_rate:.1f}%"
            )
        st.markdown("**Insight**: Graduates typically have higher admission and first-semester grades compared to dropouts, indicating academic performance as a key success factor.", unsafe_allow_html=True)

def display_risk_factors(df):
    """Display key risk factors analysis with labeled charts and insights"""
    st.markdown("<h2 style='font-size: 20px; font-weight: bold;'>‚ö†Ô∏è Key Risk Factors Analysis</h2>", unsafe_allow_html=True)
    risk_factors = []
    if 'Curricular units 1st sem (approved)' in df.columns:
        low_performance = df['Curricular units 1st sem (approved)'] <= 2
        risk_factors.append(('Low 1st Sem Performance (‚â§2 units)', low_performance.sum()))
    if 'Age at enrollment' in df.columns:
        mature_students = df['Age at enrollment'] > 25
        risk_factors.append(('Mature Students (>25 years)', mature_students.sum()))
    if 'Scholarship holder' in df.columns:
        no_scholarship = df['Scholarship holder'] == 0
        risk_factors.append(('No Financial Aid', no_scholarship.sum()))
    if 'Displaced' in df.columns:
        displaced = df['Displaced'] == 1
        risk_factors.append(('Displaced Students', displaced.sum()))

    if risk_factors:
        risk_df = pd.DataFrame(risk_factors, columns=['Risk Factor', 'Number of Students'])
        fig, ax = plt.subplots(figsize=(8, 5))
        bars = ax.bar(risk_df['Risk Factor'], risk_df['Number of Students'],
                     color=['red', 'orange', 'yellow', 'coral'][:len(risk_factors)])
        ax.set_title('Number of Students by Risk Factor', fontsize=14, fontweight='bold', pad=15)
        ax.set_ylabel('Number of Students', fontsize=12)
        ax.set_xlabel('Risk Factors', fontsize=12)
        ax.tick_params(axis='both', labelsize=10)
        plt.xticks(rotation=45, ha='right')
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width() / 2., height,
                   f'{int(height)}', ha='center', va='bottom', fontsize=10)
        plt.tight_layout()
        st.pyplot(fig)
        st.markdown("**Insight**: Risk factors like low academic performance and lack of financial aid significantly increase dropout probability.", unsafe_allow_html=True)
        plt.close()

def display_data_structure(df):
    """Display data sample and structure"""
    st.markdown("<h2 style='font-size: 20px; font-weight: bold;'>üìã Data Sample & Structure</h2>", unsafe_allow_html=True)
    col1, col2 = st.columns([2, 1])
    with col1:
        st.markdown("<h3 style='font-size: 16px;'>Sample of Student Records</h3>", unsafe_allow_html=True)
        sample_df = df.head(10)
        def highlight_outcome(val):
            if val == 'Dropout':
                return 'background-color: #ffcccc'
            elif val == 'Graduate':
                return 'background-color: #ccffcc'
            else:
                return 'background-color: #ffffcc'
        styled_df = sample_df.style.map(highlight_outcome, subset=['Target'])
        st.dataframe(styled_df)
        st.markdown("**Insight**: This sample shows key student attributes, with outcomes highlighted for quick reference.", unsafe_allow_html=True)
    with col2:
        st.markdown("<h3 style='font-size: 16px;'>Dataset Composition</h3>", unsafe_allow_html=True)
        st.write(f"‚Ä¢ **Rows:** {df.shape[0]:,} students")
        st.write(f"‚Ä¢ **Columns:** {df.shape[1]} features")
        demographic_cols = ['Gender', 'Age at enrollment', 'Marital status', 'Nacionality']
        academic_cols = [col for col in df.columns if 'grade' in col.lower() or 'units' in col.lower()]
        economic_cols = ['Unemployment rate', 'Inflation rate', 'GDP']
        st.write("**Feature Categories:**")
        st.write(f"‚Ä¢ Demographics: {len([c for c in demographic_cols if c in df.columns])}")
        st.write(f"‚Ä¢ Academic: {len([c for c in academic_cols if c in df.columns])}")
        st.write(f"‚Ä¢ Economic: {len([c for c in economic_cols if c in df.columns])}")
        st.write(
            f"‚Ä¢ Other: {df.shape[1] - len([c for c in demographic_cols + academic_cols + economic_cols if c in df.columns])}")
        st.markdown("**Insight**: The dataset includes a mix of demographic, academic, and economic features critical for predicting dropout risk.", unsafe_allow_html=True)

def display_feature_explorer(df):
    """Interactive feature explorer with labeled insights"""
    st.markdown("<h2 style='font-size: 20px; font-weight: bold;'>üîß Interactive Feature Explorer</h2>", unsafe_allow_html=True)
    col_to_explore = st.selectbox(
        "Select a feature to explore in detail:",
        options=[col for col in df.columns if col != 'Target' and col != 'id']
    )
    if col_to_explore:
        col1, col2 = st.columns([1, 1])
        with col1:
            st.markdown(f"<h3 style='font-size: 16px;'>Statistics for {col_to_explore}</h3>", unsafe_allow_html=True)
            if df[col_to_explore].dtype in ['int64', 'float64']:
                stats = df[col_to_explore].describe()
                for stat, value in stats.items():
                    st.write(f"‚Ä¢ **{stat}:** {value:.2f}")
                st.markdown(f"**Insight**: The distribution of {col_to_explore} shows its variability and central tendency.", unsafe_allow_html=True)
            else:
                unique_vals = df[col_to_explore].nunique()
                st.write(f"‚Ä¢ **Unique values:** {unique_vals}")
                st.write(f"‚Ä¢ **Most common:** {df[col_to_explore].mode().iloc[0]}")
                st.markdown(f"**Insight**: Categorical feature with {unique_vals} unique values, indicating diversity in {col_to_explore}.", unsafe_allow_html=True)
        with col2:
            st.markdown(f"<h3 style='font-size: 16px;'>{col_to_explore} vs Dropout Rate</h3>", unsafe_allow_html=True)
            if df[col_to_explore].dtype in ['int64', 'float64']:
                correlation = df[col_to_explore].corr(df['Target'].map({'Graduate': 1, 'Dropout': 0, 'Enrolled': 2}))
                st.write(f"‚Ä¢ **Correlation with dropout:** {correlation:.3f}")
                df_temp = df.copy()
                try:
                    df_temp[f'{col_to_explore}_binned'] = pd.cut(df_temp[col_to_explore], bins=5)
                    dropout_by_bin = df_temp.groupby(f'{col_to_explore}_binned', observed=True)['Target'].apply(
                        lambda x: (x == 'Dropout').mean() * 100 if len(x) > 0 else 0
                    ).round(1)
                    fig, ax = plt.subplots(figsize=(8, 5))
                    dropout_by_bin.plot(kind='bar', ax=ax, color='coral')
                    ax.set_title(f'Dropout Rate by {col_to_explore} Range', fontsize=14, fontweight='bold', pad=15)
                    ax.set_xlabel(f'{col_to_explore} Bins', fontsize=12)
                    ax.set_ylabel('Dropout Rate (%)', fontsize=12)
                    ax.tick_params(axis='both', labelsize=10)
                    plt.xticks(rotation=45, ha='right')
                    plt.tight_layout()
                    st.pyplot(fig)
                    st.markdown(f"**Insight**: Variations in {col_to_explore} show differing dropout risks across its range.", unsafe_allow_html=True)
                    plt.close()
                except Exception as e:
                    st.write(f"‚Ä¢ Unable to create bins for this feature: {str(e)}")
            else:
                dropout_by_cat = df.groupby(col_to_explore, observed=True)['Target'].apply(
                    lambda x: (x == 'Dropout').mean() * 100 if len(x) > 0 else 0
                ).round(1)
                fig, ax = plt.subplots(figsize=(8, 5))
                dropout_by_cat.plot(kind='bar', ax=ax, color='coral')
                ax.set_title(f'Dropout Rate by {col_to_explore}', fontsize=14, fontweight='bold', pad=15)
                ax.set_xlabel(col_to_explore, fontsize=12)
                ax.set_ylabel('Dropout Rate (%)', fontsize=12)
                ax.tick_params(axis='both', labelsize=10)
                plt.xticks(rotation=45, ha='right')
                plt.tight_layout()
                st.pyplot(fig)
                st.markdown(f"**Insight**: Different categories of {col_to_explore} have varying dropout rates, highlighting its impact.", unsafe_allow_html=True)
                plt.close()

def display_column_info(df):
    """Display detailed column information using tabs instead of nested expanders"""

    st.subheader("üìã Detailed Column Information")

    # Create tabs for different sections
    tab1, tab2, tab3, tab4 = st.tabs(["üìä Overview", "üìà Statistics", "‚ùå Missing Data", "üîç Data Types"])

    with tab1:
        st.write("**Dataset Overview**")
        col1, col2, col3 = st.columns(3)

        with col1:
            st.metric("Total Rows", len(df))
        with col2:
            st.metric("Total Columns", len(df.columns))
        with col3:
            st.metric("Memory Usage", f"{df.memory_usage().sum() / 1024 ** 2:.2f} MB")

        st.write("**Column Names**")
        st.write(", ".join(df.columns.tolist()))

    with tab2:
        st.write("**Statistical Summary**")

        # Numeric columns statistics
        numeric_cols = df.select_dtypes(include=['number']).columns
        if len(numeric_cols) > 0:
            st.write("*Numeric Columns:*")
            st.dataframe(df[numeric_cols].describe())

        # Categorical columns statistics
        categorical_cols = df.select_dtypes(include=['object']).columns
        if len(categorical_cols) > 0:
            st.write("*Categorical Columns:*")
            cat_stats = []
            for col in categorical_cols:
                cat_stats.append({
                    'Column': col,
                    'Unique Values': df[col].nunique(),
                    'Most Frequent': df[col].mode().iloc[0] if not df[col].mode().empty else 'N/A',
                    'Frequency': df[col].value_counts().iloc[0] if len(df[col].value_counts()) > 0 else 0
                })
            st.dataframe(cat_stats)

    with tab3:
        st.write("**Missing Data Analysis**")

        missing_data = df.isnull().sum()
        missing_percentage = (missing_data / len(df)) * 100

        missing_df = pd.DataFrame({
            'Column': missing_data.index.tolist(),
            'Missing Count': missing_data.values.tolist(),
            'Missing Percentage': missing_percentage.values.tolist()
        })

        # Only show columns with missing data
        missing_df = missing_df[missing_df['Missing Count'] > 0]

        if len(missing_df) > 0:
            st.dataframe(missing_df)

            # Visual representation
            if len(missing_df) <= 10:  # Only show chart if not too many columns
                import plotly.express as px
                fig = px.bar(missing_df, x='Column', y='Missing Percentage',
                             title='Missing Data Percentage by Column')
                fig.update_xaxis(tickangle=45)
                st.plotly_chart(fig, use_container_width=True)
        else:
            st.success("‚úÖ No missing data found in the dataset!")

    with tab4:
        st.write("**Data Types Information**")

        dtype_info = pd.DataFrame({
            'Column': df.columns.tolist(),
            'Data Type': [str(dtype) for dtype in df.dtypes],
            'Non-Null Count': df.count().tolist(),
            'Null Count': df.isnull().sum().tolist()
        })

        st.dataframe(dtype_info)

        # Data type distribution
        dtype_counts = df.dtypes.value_counts()
        col1, col2 = st.columns([1, 1])

        with col1:
            st.write("**Data Type Distribution**")
            for dtype, count in dtype_counts.items():
                st.write(f"‚Ä¢ {dtype}: {count} columns")

        with col2:
            if len(dtype_counts) > 1:
                import plotly.express as px
                # Convert to standard Python types to avoid JSON serialization error
                fig = px.pie(values=dtype_counts.values.tolist(),
                             names=[str(name) for name in dtype_counts.index],
                             title='Distribution of Data Types')
                st.plotly_chart(fig, use_container_width=True)

def load_data(uploaded_file=None):
    """Load data from file or sample dataset"""
    potential_paths = [
        'C:/Users/swapn/StudentDropoutPrediction/.venv/data/student_dropout_data.csv',
        'C:/Users/swapn/StudentDropoutPrediction/data/student_dropout_data.csv',
        './data/student_dropout_data.csv',
        '../data/student_dropout_data.csv'
    ]

    if uploaded_file is not None:
        try:
            df = pd.read_csv(uploaded_file)
            st.success("Successfully loaded uploaded file!")
            return df
        except Exception as e:
            st.error(f"Error loading uploaded file: {e}")

    for path in potential_paths:
        try:
            df = pd.read_csv(path)
            st.success(f"Successfully loaded data from {path}")
            return df
        except FileNotFoundError:
            continue
        except Exception as e:
            st.error(f"Error loading file from {path}: {e}")

    st.warning("No dataset found. Creating a minimal sample dataset.")
    df = pd.DataFrame({
        'id': [0, 1, 2],
        'Marital status': [1, 1, 1],
        'Application mode': [1, 17, 17],
        'Application order': [1, 1, 2],
        'Course': [9238, 9238, 9254],
        'Daytime/evening attendance': [1, 1, 1],
        'Previous qualification': [1, 1, 1],
        'Previous qualification (grade)': [126.0, 125.0, 137.0],
        'Nacionality': [1, 1, 1],
        'Mother\'s qualification': [1, 19, 3],
        'Father\'s qualification': [19, 19, 19],
        'Mother\'s occupation': [5, 9, 2],
        'Father\'s occupation': [5, 9, 3],
        'Admission grade': [122.6, 119.8, 144.7],
        'Displaced': [0, 1, 0],
        'Educational special needs': [0, 0, 0],
        'Debtor': [0, 0, 0],
        'Tuition fees up to date': [1, 1, 1],
        'Gender': [0, 0, 1],
        'Scholarship holder': [1, 0, 0],
        'Age at enrollment': [18, 18, 18],
        'International': [0, 0, 0],
        'Curricular units 1st sem (credited)': [0, 0, 0],
        'Curricular units 1st sem (enrolled)': [6, 6, 6],
        'Curricular units 1st sem (evaluations)': [6, 8, 0],
        'Curricular units 1st sem (approved)': [6, 4, 0],
        'Curricular units 1st sem (grade)': [14.5, 11.6, 0.0],
        'Curricular units 1st sem (without evaluations)': [0, 0, 0],
        'Curricular units 2nd sem (credited)': [0, 0, 0],
        'Curricular units 2nd sem (enrolled)': [6, 6, 6],
        'Curricular units 2nd sem (evaluations)': [7, 9, 0],
        'Curricular units 2nd sem (approved)': [6, 0, 0],
        'Curricular units 2nd sem (grade)': [12.428571428571429, 0.0, 0.0],
        'Curricular units 2nd sem (without evaluations)': [0, 0, 0],
        'Unemployment rate': [11.1, 11.1, 16.2],
        'Inflation rate': [0.6, 0.6, 0.3],
        'GDP': [2.02, 2.02, -0.92],
        'Target': ['Graduate', 'Dropout', 'Dropout']
    })
    return df

def preprocess_data(df):
    """Preprocess the data for model training"""
    processed_df = df.copy()
    if 'id' in processed_df.columns:
        processed_df = processed_df.drop('id', axis=1)
    processed_df = processed_df.fillna(processed_df.median(numeric_only=True))
    le = LabelEncoder()
    categorical_cols = processed_df.select_dtypes(include=['object']).columns.tolist()
    categorical_cols = [col for col in categorical_cols if col != 'Target']
    for col in categorical_cols:
        processed_df[col] = le.fit_transform(processed_df[col])
    if 'Target' in processed_df.columns:
        processed_df['Target'] = processed_df['Target'].map(TARGET_MAPPING)
        processed_df['Target'] = processed_df['Target'].fillna(0)
    for col in processed_df.columns:
        processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce')
    processed_df.dropna(inplace=True)
    X = processed_df.drop('Target', axis=1)
    y = processed_df['Target']
    return X, y, processed_df

def train_model(X_train, y_train):
    """Train a Random Forest Classifier"""
    progress_bar = st.progress(0)
    status_text = st.empty()
    status_text.text("Starting model training...")
    model = RandomForestClassifier(
        n_estimators=200,
        max_depth=None,
        min_samples_split=5,
        min_samples_leaf=2,
        class_weight='balanced',
        random_state=42,
        n_jobs=-1
    )
    status_text.text("Fitting the model...")
    progress_bar.progress(25)
    model.fit(X_train, y_train)
    progress_bar.progress(100)
    status_text.text("Model training complete!")
    return model

def individual_dropout_prediction(model, X):
    """Create interactive widgets for individual dropout prediction"""
    st.markdown("<h2 style='font-size: 20px; font-weight: bold;'>Student Dropout Probability Predictor</h2>", unsafe_allow_html=True)
    show_debug = st.checkbox("Show debug information")
    if show_debug:
        st.markdown("<h3 style='font-size: 16px;'>Model Information</h3>", unsafe_allow_html=True)
        st.write("Model Classes:", model.classes_)
        reverse_mapping = {v: k for k, v in TARGET_MAPPING.items()}
        st.write("Class mapping:", {f"Class {k}": reverse_mapping.get(k, f"Unknown-{k}") for k in model.classes_})
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown("<h3 style='font-size: 16px;'>Demographics</h3>", unsafe_allow_html=True)
        gender = st.selectbox("Gender", ["Female", "Male"])
        age = st.slider("Age at enrollment", min_value=17, max_value=70, value=20)
        marital_status = st.selectbox("Marital status", [1, 2, 3, 4, 5, 6])
        international = st.selectbox("International", [0, 1])
        displaced = st.selectbox("Displaced", [0, 1])
        educational_needs = st.selectbox("Educational special needs", [0, 1])
        scholarship = st.selectbox("Scholarship holder", [0, 1])
    with col2:
        st.markdown("<h3 style='font-size: 16px;'>Academic Background</h3>", unsafe_allow_html=True)
        application_mode = st.selectbox("Application mode", list(range(1, 18)))
        application_order = st.slider("Application order", min_value=1, max_value=9, value=1)
        course = st.number_input("Course Code", min_value=9000, max_value=9999, value=9238)
        attendance = st.selectbox("Daytime/evening attendance", [0, 1])
        prev_qualification = st.selectbox("Previous qualification", list(range(1, 18)))
        prev_grade = st.slider("Previous qualification grade", min_value=0.0, max_value=200.0, value=120.0, step=5.0)
        admission_grade = st.slider("Admission grade", min_value=0.0, max_value=200.0, value=120.0, step=5.0)
    with col3:
        st.markdown("<h3 style='font-size: 16px;'>Academic Performance</h3>", unsafe_allow_html=True)
        units_1st_approved = st.slider("Units approved (1st sem)", min_value=0, max_value=20, value=5)
        units_1st_grade = st.slider("Average grade (1st sem)", min_value=0.0, max_value=20.0, value=12.0, step=1.0)
        units_2nd_approved = st.slider("Units approved (2nd sem)", min_value=0, max_value=20, value=5)
        units_2nd_grade = st.slider("Average grade (2nd sem)", min_value=0.0, max_value=20.0, value=12.0, step=1.0)
    st.markdown("<h3 style='font-size: 16px;'>Economic Indicators</h3>", unsafe_allow_html=True)
    eco_col1, eco_col2, eco_col3 = st.columns(3)
    with eco_col1:
        unemployment = st.slider("Unemployment rate", min_value=0.0, max_value=20.0, value=11.0, step=1.0)
    with eco_col2:
        inflation = st.slider("Inflation rate", min_value=-2.0, max_value=5.0, value=0.6, step=0.5)
    with eco_col3:
        gdp = st.slider("GDP", min_value=-5.0, max_value=5.0, value=2.0, step=0.5)
    default_values = {
        'Mother\'s qualification': 10,
        'Father\'s qualification': 10,
        'Mother\'s occupation': 5,
        'Father\'s occupation': 5,
        'Debtor': 0,
        'Tuition fees up to date': 1,
        'Nacionality': 1,
        'Curricular units 1st sem (credited)': 0,
        'Curricular units 1st sem (enrolled)': 6,
        'Curricular units 1st sem (evaluations)': 6,
        'Curricular units 1st sem (without evaluations)': 0,
        'Curricular units 2nd sem (credited)': 0,
        'Curricular units 2nd sem (enrolled)': 6,
        'Curricular units 2nd sem (evaluations)': 6,
        'Curricular units 2nd sem (without evaluations)': 0,
    }
    show_advanced = st.checkbox("Show advanced options")
    if show_advanced:
        st.markdown("<h3 style='font-size: 16px;'>Advanced Features</h3>", unsafe_allow_html=True)
        col_adv1, col_adv2 = st.columns(2)
        with col_adv1:
            for key in list(default_values.keys())[:len(default_values) // 2]:
                if key in ['Nacionality', 'Debtor', 'Tuition fees up to date']:
                    default_values[key] = st.selectbox(key, [0, 1], index=default_values[key])
                else:
                    default_values[key] = st.number_input(key, value=default_values[key])
        with col_adv2:
            for key in list(default_values.keys())[len(default_values) // 2:]:
                default_values[key] = st.number_input(key, value=default_values[key])
    if st.button("Predict Dropout Probability"):
        with st.spinner('Calculating prediction...'):
            input_dict = {
                'Marital status': marital_status,
                'Application mode': application_mode,
                'Application order': application_order,
                'Course': course,
                'Daytime/evening attendance': attendance,
                'Previous qualification': prev_qualification,
                'Previous qualification (grade)': prev_grade,
                'Admission grade': admission_grade,
                'Displaced': displaced,
                'Educational special needs': educational_needs,
                'Gender': 1 if gender == "Male" else 0,
                'Scholarship holder': scholarship,
                'Age at enrollment': age,
                'International': international,
                'Curricular units 1st sem (approved)': units_1st_approved,
                'Curricular units 1st sem (grade)': units_1st_grade,
                'Curricular units 2nd sem (approved)': units_2nd_approved,
                'Curricular units 2nd sem (grade)': units_2nd_grade,
                'Unemployment rate': unemployment,
                'Inflation rate': inflation,
                'GDP': gdp
            }
            for key, value in default_values.items():
                input_dict[key] = value
            input_data = pd.DataFrame([input_dict])
            if show_debug:
                st.markdown("<h3 style='font-size: 16px;'>Debug Information</h3>", unsafe_allow_html=True)
                st.write("Raw Input Data:")
                st.write(input_data)
            try:
                for col in X.columns:
                    if col not in input_data.columns:
                        input_data[col] = X[col].median()
                input_data = input_data[X.columns]
                prediction_probs = model.predict_proba(input_data)[0]
                reverse_mapping = {v: k for k, v in TARGET_MAPPING.items()}
                class_names = [reverse_mapping.get(i, f"Class {i}") for i in model.classes_]
                results = pd.DataFrame({
                    'Outcome': class_names,
                    'Probability': [f"{p * 100:.2f}%" for p in prediction_probs],
                    'Raw Value': prediction_probs
                })
                st.markdown("<h3 style='font-size: 16px;'>Prediction Results</h3>", unsafe_allow_html=True)
                st.dataframe(results)
                for outcome, prob in zip(class_names, prediction_probs):
                    st.write(f"{outcome}: {prob:.2%}")
                    st.progress(float(prob))
                max_class_idx = np.argmax(prediction_probs)
                max_class_name = class_names[max_class_idx]
                max_prob = prediction_probs[max_class_idx]
                st.markdown("<h3 style='font-size: 16px;'>Prediction Summary</h3>", unsafe_allow_html=True)
                if max_class_name == "Dropout":
                    st.warning(f"‚ö†Ô∏è Risk of student dropout detected! Probability: {max_prob:.2%}")
                    st.write("Recommended actions:")
                    st.write("- Schedule academic counseling")
                    st.write("- Review financial aid options")
                    st.write("- Consider tutoring or additional support")
                else:
                    st.success(f"‚úÖ Student likely to succeed! Probability of {max_class_name}: {max_prob:.2%}")
                    st.write("Recommended actions:")
                    st.write("- Continue regular monitoring")
                    st.write("- Provide ongoing encouragement and support")
                st.markdown(f"**Insight**: The model predicts a {max_prob:.2%} chance of {max_class_name}, guiding targeted interventions.", unsafe_allow_html=True)
            except Exception as e:
                st.error(f"Prediction error: {str(e)}")
                if show_debug:
                    import traceback
                    st.code(traceback.format_exc())

def visualize_model_results(model, X_test, y_test):
    """Visualize model evaluation results with labeled charts and insights"""
    st.markdown("<h2 style='font-size: 20px; font-weight: bold;'>Model Evaluation Results</h2>", unsafe_allow_html=True)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    st.metric("Model Accuracy", f"{accuracy:.2%}")
    st.markdown(f"**Insight**: The model achieves {accuracy:.2%} accuracy in predicting student outcomes.", unsafe_allow_html=True)
    display_mode = st.radio("Select display mode:", ["Tabs", "Panels"], index=1)
    if display_mode == "Tabs":
        tabs = st.tabs(["Classification Report", "Confusion Matrix", "Feature Importance"])
        with tabs[0]:
            st.markdown("<h3 style='font-size: 16px;'>Classification Report</h3>", unsafe_allow_html=True)
            report = classification_report(y_test, y_pred, output_dict=True)
            report_df = pd.DataFrame(report).transpose()
            st.dataframe(report_df)
            st.markdown("**Insight**: Precision, recall, and F1-scores show the model's performance across Graduate, Dropout, and Enrolled classes.", unsafe_allow_html=True)
        with tabs[1]:
            st.markdown("<h3 style='font-size: 16px;'>Confusion Matrix</h3>", unsafe_allow_html=True)
            cm = confusion_matrix(y_test, y_pred)
            fig, ax = plt.subplots(figsize=(8, 5))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                       annot_kws={"size": 10}, cbar_kws={"shrink": 0.8})
            reverse_mapping = {0: 'Dropout', 1: 'Graduate', 2: 'Enrolled'}
            class_labels = [reverse_mapping.get(c, f"Class {c}") for c in sorted(np.unique(y_test))]
            ax.set_title('Confusion Matrix of Model Predictions', fontsize=14, fontweight='bold', pad=15)
            ax.set_ylabel('Actual Outcome', fontsize=12)
            ax.set_xlabel('Predicted Outcome', fontsize=12)
            ax.set_xticklabels(class_labels, fontsize=10, rotation=45, ha='right')
            ax.set_yticklabels(class_labels, fontsize=10)
            plt.tight_layout()
            st.pyplot(fig)
            st.markdown("**Insight**: The matrix shows correct and incorrect predictions, with higher diagonal values indicating better performance.", unsafe_allow_html=True)
            plt.close()
        with tabs[2]:
            st.markdown("<h3 style='font-size: 16px;'>Feature Importance</h3>", unsafe_allow_html=True)
            feature_importance = pd.DataFrame({
                'Feature': X_test.columns,
                'Importance': model.feature_importances_
            }).sort_values('Importance', ascending=False)
            fig, ax = plt.subplots(figsize=(8, 5))
            top_features = feature_importance.head(15)
            sns.barplot(x='Importance', y='Feature', data=top_features, ax=ax, color='skyblue')
            ax.set_title('Top 15 Features Influencing Dropout Prediction', fontsize=14, fontweight='bold', pad=15)
            ax.set_xlabel('Feature Importance Score', fontsize=12)
            ax.set_ylabel('Features', fontsize=12)
            ax.tick_params(axis='both', labelsize=10)
            plt.tight_layout()
            st.pyplot(fig)
            st.markdown("**Insight**: Features like first-semester grades and units approved are critical drivers of the model's predictions.", unsafe_allow_html=True)
            plt.close()
    else:
        st.markdown("<h3 style='font-size: 16px;'>Classification Report</h3>", unsafe_allow_html=True)
        report = classification_report(y_test, y_pred, output_dict=True)
        report_df = pd.DataFrame(report).transpose()
        st.dataframe(report_df)
        st.markdown("**Insight**: Precision, recall, and F1-scores show the model's performance across Graduate, Dropout, and Enrolled classes.", unsafe_allow_html=True)
        col1, col2 = st.columns([1, 1])
        with col1:
            st.markdown("<h3 style='font-size: 16px;'>Confusion Matrix</h3>", unsafe_allow_html=True)
            cm = confusion_matrix(y_test, y_pred)
            fig, ax = plt.subplots(figsize=(8, 5))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                       annot_kws={"size": 10}, cbar_kws={"shrink": 0.8})
            reverse_mapping = {0: 'Dropout', 1: 'Graduate', 2: 'Enrolled'}
            class_labels = [reverse_mapping.get(c, f"Class {c}") for c in sorted(np.unique(y_test))]
            ax.set_title('Confusion Matrix of Model Predictions', fontsize=14, fontweight='bold', pad=15)
            ax.set_ylabel('Actual Outcome', fontsize=12)
            ax.set_xlabel('Predicted Outcome', fontsize=12)
            ax.set_xticklabels(class_labels, fontsize=10, rotation=45, ha='right')
            ax.set_yticklabels(class_labels, fontsize=10)
            plt.tight_layout()
            st.pyplot(fig)
            st.markdown("**Insight**: The matrix shows correct and incorrect predictions, with higher diagonal values indicating better performance.", unsafe_allow_html=True)
            plt.close()
        with col2:
            st.markdown("<h3 style='font-size: 16px;'>Feature Importance</h3>", unsafe_allow_html=True)
            feature_importance = pd.DataFrame({
                'Feature': X_test.columns,
                'Importance': model.feature_importances_
            }).sort_values('Importance', ascending=False)
            fig, ax = plt.subplots(figsize=(8, 5))
            top_features = feature_importance.head(15)
            sns.barplot(x='Importance', y='Feature', data=top_features, ax=ax, color='skyblue')
            ax.set_title('Top 15 Features Influencing Dropout Prediction', fontsize=14, fontweight='bold', pad=15)
            ax.set_xlabel('Feature Importance Score', fontsize=12)
            ax.set_ylabel('Features', fontsize=12)
            ax.tick_params(axis='both', labelsize=10)
            plt.tight_layout()
            st.pyplot(fig)
            st.markdown("**Insight**: Features like first-semester grades and units approved are critical drivers of the model's predictions.", unsafe_allow_html=True)
            plt.close()

def display_global_feature_importance(model, feature_names):
    """Display global feature importance using multiple methods"""
    st.markdown("#### üåç Global Feature Importance")
    st.markdown("Shows which features are most important across all predictions in the dataset.")

    # Method selection
    importance_method = st.selectbox(
        "Select Importance Method",
        ["Built-in Feature Importance", "SHAP Global Importance", "Permutation Importance"]
    )

    try:
        if importance_method == "Built-in Feature Importance":
            # For tree-based models
            if hasattr(model, 'feature_importances_'):
                importances = model.feature_importances_

                # Create DataFrame for plotting
                importance_df = pd.DataFrame({
                    'Feature': feature_names,
                    'Importance': importances
                }).sort_values('Importance', ascending=False)

                # Plot using plotly for interactivity
                fig = px.bar(
                    importance_df.head(15),
                    x='Importance',
                    y='Feature',
                    orientation='h',
                    title="Top 15 Most Important Features",
                    color='Importance',
                    color_continuous_scale='viridis'
                )
                fig.update_layout(yaxis={'categoryorder': 'total ascending'})
                st.plotly_chart(fig, use_container_width=True)

                # Show top features summary
                st.markdown("**üìä Top 5 Most Important Features:**")
                for i, (idx, row) in enumerate(importance_df.head(5).iterrows()):
                    st.write(f"{i + 1}. **{row['Feature']}**: {row['Importance']:.4f}")

                st.markdown(
                    "**Insight**: These features have the highest impact on the model's decision-making process globally.")

            else:
                st.warning("This model doesn't have built-in feature importance. Try SHAP or Permutation Importance.")

        elif importance_method == "SHAP Global Importance":
            with st.spinner("Calculating SHAP values... This may take a moment."):
                # Use a sample for SHAP calculation to speed up
                sample_size = min(100, len(st.session_state.X_train))
                X_sample = st.session_state.X_train.sample(n=sample_size, random_state=42)

                # Ensure feature_names is the right type and length
                if hasattr(feature_names, 'tolist'):
                    feature_names_list = feature_names.tolist()
                else:
                    feature_names_list = list(feature_names)

                # CRITICAL FIX: Ensure X_sample and feature_names match
                if len(feature_names_list) != X_sample.shape[1]:
                    st.error(
                        f"Feature names length ({len(feature_names_list)}) doesn't match data columns ({X_sample.shape[1]})")
                    return

                # Create SHAP explainer
                explainer = shap.TreeExplainer(model)
                shap_values = explainer.shap_values(X_sample)

                # Debug information
                st.write(f"Debug - Feature names length: {len(feature_names_list)}")
                st.write(f"Debug - X_sample shape: {X_sample.shape}")

                # FIXED: Handle multi-class SHAP values properly
                if isinstance(shap_values, list):
                    # Multi-class case: shap_values is a list of arrays
                    st.write(f"Debug - Number of classes: {len(shap_values)}")
                    st.write(f"Debug - SHAP values shape per class: {shap_values[0].shape}")

                    # FIXED: Calculate mean absolute SHAP values properly
                    # Each element in shap_values is shape (n_samples, n_features)

                    # Method: Average absolute SHAP values across samples for each class, then across classes
                    class_importances = []
                    for class_shap in shap_values:
                        # Take mean absolute SHAP value across samples for this class
                        class_importance = np.abs(class_shap).mean(axis=0)  # Shape: (n_features,)
                        class_importances.append(class_importance)

                    # Stack class importances and average across classes
                    class_importances = np.array(class_importances)  # Shape: (n_classes, n_features)
                    mean_shap = np.mean(class_importances, axis=0)  # Shape: (n_features,)

                    # Use the first class for detailed plotting
                    shap_values_plot = shap_values[0]

                elif shap_values.ndim == 3:
                    # Alternative multi-class format: shape (n_samples, n_features, n_classes)
                    st.write(f"Debug - SHAP values shape (3D): {shap_values.shape}")

                    # Take mean absolute SHAP values across samples, then across classes
                    # shap_values shape: (n_samples, n_features, n_classes)
                    mean_abs_shap_per_class = np.abs(shap_values).mean(axis=0)  # Shape: (n_features, n_classes)
                    mean_shap = np.mean(mean_abs_shap_per_class, axis=1)  # Shape: (n_features,)

                    # Use the first class for plotting (convert to list format for compatibility)
                    shap_values_plot = shap_values[:, :, 0]  # Shape: (n_samples, n_features)

                else:
                    # Binary classification case
                    st.write(f"Debug - SHAP values shape: {shap_values.shape}")
                    shap_values_plot = shap_values
                    mean_shap = np.abs(shap_values).mean(axis=0)

                # Ensure mean_shap is 1-dimensional and matches feature count
                if mean_shap.ndim > 1:
                    mean_shap = mean_shap.flatten()

                st.write(f"Debug - Mean SHAP shape: {mean_shap.shape}")
                st.write(f"Debug - Expected features: {len(feature_names_list)}")

                # FINAL CHECK: Ensure lengths match
                if len(feature_names_list) != len(mean_shap):
                    st.error(
                        f"FINAL CHECK - Length mismatch: {len(feature_names_list)} features vs {len(mean_shap)} SHAP values")
                    st.error("This suggests a fundamental issue with the data preprocessing or model training.")
                    return

                # Create DataFrame
                shap_df = pd.DataFrame({
                    'Feature': feature_names_list,
                    'Mean_SHAP_Value': mean_shap
                }).sort_values('Mean_SHAP_Value', ascending=False)

                # Plot
                fig = px.bar(
                    shap_df.head(15),
                    x='Mean_SHAP_Value',
                    y='Feature',
                    orientation='h',
                    title="SHAP Feature Importance (Mean Absolute SHAP Values)",
                    color='Mean_SHAP_Value',
                    color_continuous_scale='plasma'
                )
                fig.update_layout(yaxis={'categoryorder': 'total ascending'})
                st.plotly_chart(fig, use_container_width=True)

                # Show top features summary
                st.markdown("**üìä Top 5 Most Important Features (SHAP):**")
                for i, (idx, row) in enumerate(shap_df.head(5).iterrows()):
                    st.write(f"{i + 1}. **{row['Feature']}**: {row['Mean_SHAP_Value']:.4f}")

                # SHAP Summary Plot - FIXED
                st.markdown("**üéØ SHAP Summary Plot**")
                try:
                    # Create figure with proper size
                    fig_shap, ax = plt.subplots(figsize=(12, 8))

                    # Ensure feature names are numpy array
                    feature_names_array = np.array(feature_names_list)

                    # FIXED: Use correct parameters for summary plot
                    shap.summary_plot(
                        shap_values_plot,  # Shape: (n_samples, n_features)
                        X_sample,  # Shape: (n_samples, n_features)
                        feature_names=feature_names_array,
                        show=False,
                        max_display=15,
                        plot_size=(12, 8)
                    )

                    if isinstance(shap_values, list) or shap_values.ndim == 3:
                        st.markdown(
                            "*Note: Summary plot shows SHAP values for the first class (typically dropout risk)*")

                    # Display the plot
                    st.pyplot(fig_shap, clear_figure=True)

                    st.markdown(
                        "**Insight**: SHAP values show both the magnitude and direction of feature impact. "
                        "Red dots indicate higher feature values, blue dots indicate lower values.")

                except Exception as plot_error:
                    st.warning(f"Could not generate SHAP summary plot: {str(plot_error)}")
                    st.info("The feature importance values above are still valid.")

                    # Additional debugging information
                    st.write("Debug info for summary plot:")
                    st.write(f"- shap_values_plot shape: {shap_values_plot.shape}")
                    st.write(f"- X_sample shape: {X_sample.shape}")
                    st.write(f"- feature_names_array length: {len(feature_names_array)}")

                    # Try alternative visualization
                    st.markdown("**Alternative: Top Feature Contributions**")
                    try:
                        # Create a simple bar plot of mean absolute SHAP values
                        top_features = shap_df.head(10)
                        fig_alt = px.bar(
                            top_features,
                            x='Mean_SHAP_Value',
                            y='Feature',
                            orientation='h',
                            title="Top 10 Feature Contributions (SHAP)",
                            color='Mean_SHAP_Value',
                            color_continuous_scale='RdYlBu_r'
                        )
                        fig_alt.update_layout(yaxis={'categoryorder': 'total ascending'})
                        st.plotly_chart(fig_alt, use_container_width=True)
                    except Exception as alt_error:
                        st.error(f"Alternative visualization also failed: {str(alt_error)}")

        elif importance_method == "Permutation Importance":
            with st.spinner("Calculating permutation importance..."):
                # Calculate permutation importance
                perm_importance = permutation_importance(
                    model,
                    st.session_state.X_test,
                    st.session_state.y_test,
                    n_repeats=5,
                    random_state=42
                )

                # Create DataFrame
                perm_df = pd.DataFrame({
                    'Feature': feature_names,
                    'Importance': perm_importance.importances_mean,
                    'Std': perm_importance.importances_std
                }).sort_values('Importance', ascending=False)

                # Plot with error bars
                fig = px.bar(
                    perm_df.head(15),
                    x='Importance',
                    y='Feature',
                    orientation='h',
                    error_x='Std',
                    title="Permutation Feature Importance",
                    color='Importance',
                    color_continuous_scale='cividis'
                )
                fig.update_layout(yaxis={'categoryorder': 'total ascending'})
                st.plotly_chart(fig, use_container_width=True)

                # Show top features summary
                st.markdown("**üìä Top 5 Most Important Features (Permutation):**")
                for i, (idx, row) in enumerate(perm_df.head(5).iterrows()):
                    st.write(f"{i + 1}. **{row['Feature']}**: {row['Importance']:.4f} (¬±{row['Std']:.4f})")

                st.markdown(
                    "**Insight**: Permutation importance shows how much model performance decreases when each feature is randomly shuffled.")

    except Exception as e:
        st.error(f"Error calculating feature importance: {str(e)}")
        st.info("This might happen with certain model types. Try a different importance method.")

        # Add debugging information
        if importance_method == "SHAP Global Importance":
            st.info(
                "SHAP errors often occur with multi-class models. The built-in feature importance or permutation importance methods should work reliably.")

            # Additional debugging
            st.write("**Debug Information:**")
            if 'X_train' in st.session_state:
                st.write(f"- X_train shape: {st.session_state.X_train.shape}")
            if 'feature_names' in locals():
                st.write(f"- Feature names length: {len(feature_names)}")
            st.write(f"- Model type: {type(model).__name__}")

def display_local_explanation(model, X_train, X_test, feature_names):
    """Display local explanations for individual predictions"""
    st.markdown("#### üîç Local Prediction Explanation")
    st.markdown("Understand why the model made a specific prediction for an individual student.")

    # Student selection
    st.markdown("**Select a student to explain:**")
    student_idx = st.selectbox(
        "Choose student index from test set",
        range(len(X_test)),
        format_func=lambda x: f"Student {x + 1}"
    )

    # Get the selected student data
    student_data = X_test.iloc[student_idx:student_idx + 1]
    prediction = model.predict(student_data)[0]
    prediction_proba = model.predict_proba(student_data)[0]

    # Display prediction
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Predicted Outcome", f"Class {prediction}")
    with col2:
        max_proba = np.max(prediction_proba)
        st.metric("Confidence", f"{max_proba:.2%}")
    with col3:
        # Map prediction to readable format if you have TARGET_MAPPING
        readable_prediction = "Dropout" if prediction == 0 else "Graduate" if prediction == 1 else "Enrolled"
        st.metric("Prediction", readable_prediction)

    # Show all class probabilities
    st.markdown("**üìä Prediction Probabilities:**")
    prob_df = pd.DataFrame({
        'Class': [f'Class {i}' for i in range(len(prediction_proba))],
        'Probability': prediction_proba
    })
    fig = px.bar(prob_df, x='Class', y='Probability',
                 title="Prediction Probabilities for Selected Student")
    st.plotly_chart(fig, use_container_width=True)

    # Explanation method selection
    explanation_method = st.selectbox(
        "Select Explanation Method",
        ["SHAP Local Explanation", "LIME Explanation"]
    )

    try:
        if explanation_method == "SHAP Local Explanation":
            with st.spinner("Generating SHAP explanation..."):
                # Create SHAP explainer
                explainer = shap.TreeExplainer(model)
                shap_values = explainer.shap_values(student_data)

                # Debug information
                st.write(f"Debug - SHAP values type: {type(shap_values)}")
                if isinstance(shap_values, list):
                    st.write(f"Debug - Number of classes: {len(shap_values)}")
                    st.write(f"Debug - SHAP values shape per class: {shap_values[0].shape}")
                elif hasattr(shap_values, 'shape'):
                    st.write(f"Debug - SHAP values shape: {shap_values.shape}")

                # Handle different SHAP value formats
                if isinstance(shap_values, list):
                    # Multi-class case: shap_values is a list of arrays
                    shap_values_for_prediction = shap_values[prediction]  # Use predicted class
                    expected_value = explainer.expected_value[prediction] if isinstance(explainer.expected_value,
                                                                                        np.ndarray) else explainer.expected_value
                elif shap_values.ndim == 3:
                    # Alternative multi-class format: shape (n_samples, n_features, n_classes)
                    shap_values_for_prediction = shap_values[0, :, prediction]  # First sample, predicted class
                    expected_value = explainer.expected_value[prediction] if isinstance(explainer.expected_value,
                                                                                        np.ndarray) else explainer.expected_value
                else:
                    # Binary classification case
                    shap_values_for_prediction = shap_values[0]  # First (and only) sample
                    expected_value = explainer.expected_value

                # Ensure we have the right shape for a single prediction
                if shap_values_for_prediction.ndim > 1:
                    shap_values_for_prediction = shap_values_for_prediction.flatten()

                st.write(f"Debug - Final SHAP values shape: {shap_values_for_prediction.shape}")
                st.write(f"Debug - Expected value: {expected_value}")
                st.write(f"Debug - Feature names length: {len(feature_names)}")

                # SHAP waterfall plot - FIXED VERSION
                st.markdown("**üåä SHAP Waterfall Plot**")

                try:
                    # Create the explanation object properly for waterfall plot
                    shap_explanation = shap.Explanation(
                        values=shap_values_for_prediction,  # 1D array of SHAP values
                        base_values=expected_value,  # Scalar base value
                        data=student_data.iloc[0].values,  # 1D array of feature values
                        feature_names=list(feature_names)  # List of feature names
                    )

                    # Create waterfall plot
                    fig_waterfall = plt.figure(figsize=(12, 8))
                    shap.plots.waterfall(shap_explanation, show=False, max_display=15)
                    st.pyplot(fig_waterfall, clear_figure=True)

                    st.markdown(f"*Note: Waterfall plot shows SHAP values for predicted class: {readable_prediction}*")

                except Exception as waterfall_error:
                    st.warning(f"Could not generate waterfall plot: {str(waterfall_error)}")
                    st.info("Showing alternative visualization instead.")

                    # Alternative: Horizontal bar chart of SHAP values
                    st.markdown("**üìä Feature Contributions (Alternative View)**")

                    # Create DataFrame for plotting
                    shap_df_alt = pd.DataFrame({
                        'Feature': feature_names,
                        'SHAP_Value': shap_values_for_prediction,
                        'Feature_Value': student_data.iloc[0].values
                    })

                    # Sort by absolute SHAP value
                    shap_df_alt['Abs_SHAP'] = np.abs(shap_df_alt['SHAP_Value'])
                    shap_df_alt = shap_df_alt.sort_values('Abs_SHAP', ascending=False).head(15)

                    # Create horizontal bar plot
                    fig_alt = px.bar(
                        shap_df_alt,
                        x='SHAP_Value',
                        y='Feature',
                        orientation='h',
                        color='SHAP_Value',
                        color_continuous_scale=['red', 'white', 'blue'],
                        color_continuous_midpoint=0,
                        title=f"SHAP Feature Contributions for {readable_prediction} Prediction",
                        hover_data=['Feature_Value']
                    )
                    fig_alt.update_layout(
                        yaxis={'categoryorder': 'total ascending'},
                        height=600
                    )
                    st.plotly_chart(fig_alt, use_container_width=True)

                # Top contributing features analysis
                shap_importance = pd.DataFrame({
                    'Feature': feature_names,
                    'SHAP_Value': shap_values_for_prediction,
                    'Feature_Value': student_data.iloc[0].values
                })
                shap_importance['Abs_SHAP'] = np.abs(shap_importance['SHAP_Value'])
                shap_importance = shap_importance.sort_values('Abs_SHAP', ascending=False)

                st.markdown("**üéØ Top Contributing Features:**")
                for i, (idx, row) in enumerate(shap_importance.head(5).iterrows()):
                    impact = "increases" if row['SHAP_Value'] > 0 else "decreases"
                    impact_color = "üî¥" if row['SHAP_Value'] > 0 else "üü¢"
                    st.write(
                        f"{i + 1}. {impact_color} **{row['Feature']}** (value: {row['Feature_Value']:.2f}) "
                        f"{impact} {readable_prediction.lower()} probability by {abs(row['SHAP_Value']):.4f}"
                    )

                # Summary insight
                st.markdown("**üí° Key Insights:**")
                positive_features = shap_importance[shap_importance['SHAP_Value'] > 0]
                negative_features = shap_importance[shap_importance['SHAP_Value'] < 0]

                if len(positive_features) > 0:
                    st.write(
                        f"‚Ä¢ **Risk factors**: {len(positive_features)} features increase {readable_prediction.lower()} probability")
                if len(negative_features) > 0:
                    st.write(
                        f"‚Ä¢ **Protective factors**: {len(negative_features)} features decrease {readable_prediction.lower()} probability")

        elif explanation_method == "LIME Explanation":
            with st.spinner("Generating LIME explanation..."):
                # Create LIME explainer
                explainer = lime.lime_tabular.LimeTabularExplainer(
                    X_train.values,
                    feature_names=feature_names,
                    class_names=[f'Class_{i}' for i in range(len(prediction_proba))],
                    mode='classification'
                )

                # Generate explanation
                explanation = explainer.explain_instance(
                    student_data.iloc[0].values,
                    model.predict_proba,
                    num_features=15
                )

                # Display LIME plot
                st.markdown("**üçÉ LIME Explanation**")
                fig_lime = explanation.as_pyplot_figure()
                fig_lime.set_size_inches(12, 8)
                st.pyplot(fig_lime, clear_figure=True)

                # Extract and display feature contributions
                lime_values = explanation.as_list()
                st.markdown("**üìã LIME Feature Contributions:**")
                for i, (feature_desc, contribution) in enumerate(lime_values[:8]):
                    impact = "supports" if contribution > 0 else "opposes"
                    impact_color = "üî¥" if contribution > 0 else "üü¢"
                    st.write(
                        f"{i + 1}. {impact_color} {feature_desc} {impact} the prediction (weight: {contribution:.4f})")

                # LIME explanation summary
                st.markdown("**üí° LIME Insights:**")
                st.write(f"‚Ä¢ LIME shows local feature importance around this specific prediction")
                st.write(f"‚Ä¢ Positive weights support the predicted class, negative weights oppose it")

    except Exception as e:
        st.error(f"Error generating explanation: {str(e)}")
        st.info("Try selecting a different student or explanation method.")

        # Additional debugging information
        st.write("**Debug Information:**")
        st.write(f"- Student data shape: {student_data.shape}")
        st.write(f"- Prediction: {prediction}")
        st.write(f"- Prediction probabilities: {prediction_proba}")
        st.write(f"- Feature names length: {len(feature_names)}")
        st.write(f"- Model type: {type(model).__name__}")

        if 'shap_values' in locals():
            st.write(f"- SHAP values type: {type(shap_values)}")
            if isinstance(shap_values, list):
                st.write(f"- SHAP values list length: {len(shap_values)}")
                st.write(f"- First element shape: {shap_values[0].shape}")
            elif hasattr(shap_values, 'shape'):
                st.write(f"- SHAP values shape: {shap_values.shape}")

def display_feature_impact_analysis(model, X_train, X_test, feature_names, df):
    """Display how different feature values impact predictions"""
    st.markdown("#### üìà Feature Impact Analysis")
    st.markdown("Explore how changing feature values affects prediction probabilities.")

    # Feature selection for analysis
    selected_feature = st.selectbox(
        "Select feature to analyze",
        feature_names
    )

    if selected_feature:
        # Get feature statistics
        feature_stats = df[selected_feature].describe()

        col1, col2 = st.columns([2, 1])

        with col1:
            # Create range of values for the selected feature
            feature_min = feature_stats['min']
            feature_max = feature_stats['max']
            feature_mean = feature_stats['mean']

            # Generate range of values
            if feature_max - feature_min > 0:
                feature_range = np.linspace(feature_min, feature_max, 50)
            else:
                feature_range = [feature_mean]

            # Use a representative sample from test set
            sample_student = X_test.iloc[0:1].copy()

            # Calculate predictions for different feature values
            predictions_over_range = []
            for value in feature_range:
                temp_student = sample_student.copy()
                temp_student[selected_feature] = value
                pred_proba = model.predict_proba(temp_student)[0]
                predictions_over_range.append(pred_proba)

            # Convert to DataFrame for plotting
            pred_df = pd.DataFrame(predictions_over_range)
            pred_df['Feature_Value'] = feature_range

            # Melt for plotting
            pred_melted = pred_df.melt(id_vars=['Feature_Value'],
                                       value_vars=list(range(len(pred_df.columns) - 1)),
                                       var_name='Class', value_name='Probability')
            pred_melted['Class'] = pred_melted['Class'].apply(lambda x: f'Class_{x}')

            # Plot feature impact
            fig = px.line(pred_melted, x='Feature_Value', y='Probability',
                          color='Class', title=f'Impact of {selected_feature} on Predictions')
            fig.add_vline(x=feature_mean, line_dash="dash",
                          annotation_text=f"Mean: {feature_mean:.2f}")
            fig.update_layout(
                xaxis_title=selected_feature,
                yaxis_title='Prediction Probability'
            )
            st.plotly_chart(fig, use_container_width=True)

            st.markdown(
                f"**Insight**: This shows how changing {selected_feature} from {feature_min:.2f} to {feature_max:.2f} affects the model's predictions.")

        with col2:
            st.markdown("**üìä Feature Statistics**")
            st.write(f"**Min**: {feature_stats['min']:.2f}")
            st.write(f"**Max**: {feature_stats['max']:.2f}")
            st.write(f"**Mean**: {feature_stats['mean']:.2f}")
            st.write(f"**Std**: {feature_stats['std']:.2f}")

            # Feature value distribution
            st.markdown("**üìà Value Distribution**")
            fig_hist = px.histogram(df, x=selected_feature, nbins=30)
            st.plotly_chart(fig_hist, use_container_width=True)

            # Risk zones
            st.markdown("**‚ö†Ô∏è Risk Analysis**")
            if len(feature_range) > 1:
                high_risk_threshold = np.percentile(feature_range, 75)
                low_risk_threshold = np.percentile(feature_range, 25)
                st.write(f"**High Risk Zone**: > {high_risk_threshold:.2f}")
                st.write(f"**Medium Risk Zone**: {low_risk_threshold:.2f} - {high_risk_threshold:.2f}")
                st.write(f"**Low Risk Zone**: < {low_risk_threshold:.2f}")

def individual_dropout_prediction_with_explanation(model, X, X_train, feature_names):
    """Enhanced individual prediction with explanations - FIXED VERSION"""
    st.markdown("#### üéØ Individual Student Prediction with Explanation")

    # Create input form for student data
    st.markdown("**Enter Student Information:**")

    # Create columns for better layout
    col1, col2 = st.columns(2)

    input_data = {}
    # Fix: Handle both list and pandas Index/Series cases
    if hasattr(feature_names, 'tolist'):
        feature_list = feature_names.tolist()
    else:
        feature_list = list(feature_names)  # Already a list or convert to list

    # Split features into two columns
    mid_point = len(feature_list) // 2

    with col1:
        for feature in feature_list[:mid_point]:
            if feature in X.columns:
                min_val = float(X[feature].min())
                max_val = float(X[feature].max())
                mean_val = float(X[feature].mean())

                # FIXED: Ensure scalar values are stored, not arrays
                input_value = st.number_input(
                    f"{feature}",
                    min_value=min_val,
                    max_value=max_val,
                    value=mean_val,
                    key=f"input_{feature}"
                )
                # Convert to Python scalar to avoid numpy array issues
                input_data[feature] = float(input_value)

    with col2:
        for feature in feature_list[mid_point:]:
            if feature in X.columns:
                min_val = float(X[feature].min())
                max_val = float(X[feature].max())
                mean_val = float(X[feature].mean())

                # FIXED: Ensure scalar values are stored, not arrays
                input_value = st.number_input(
                    f"{feature}",
                    min_value=min_val,
                    max_value=max_val,
                    value=mean_val,
                    key=f"input_{feature}"
                )
                # Convert to Python scalar to avoid numpy array issues
                input_data[feature] = float(input_value)

    if st.button("üîÆ Predict with Explanation", type="primary"):
        try:
            # FIXED: Create DataFrame more carefully
            # Ensure all values are scalars and in the correct order
            ordered_data = {}
            for feature in feature_list:
                if feature in input_data:
                    # Ensure it's a scalar value
                    value = input_data[feature]
                    if hasattr(value, 'item'):  # If it's a numpy scalar
                        value = value.item()
                    ordered_data[feature] = value
                else:
                    # If feature is missing, use mean from training data
                    ordered_data[feature] = float(X[feature].mean())

            # Create DataFrame with explicit scalar values
            input_df = pd.DataFrame([ordered_data])

            # Verify DataFrame shape and contents
            st.write(f"Debug - Input DataFrame shape: {input_df.shape}")
            st.write(f"Debug - Expected features: {len(feature_list)}")
            st.write(f"Debug - DataFrame columns: {len(input_df.columns)}")

            # Ensure column order matches training data
            input_df = input_df.reindex(columns=feature_list, fill_value=0)

            # Make prediction
            prediction = model.predict(input_df)[0]
            prediction_proba = model.predict_proba(input_df)[0]

            # Display results
            st.markdown("---")
            st.markdown("### üìä Prediction Results")

            col1, col2, col3 = st.columns(3)
            with col1:
                readable_prediction = "Dropout" if prediction == 0 else "Graduate" if prediction == 1 else "Enrolled"
                st.metric("üéØ Prediction", readable_prediction)

            with col2:
                confidence = np.max(prediction_proba)
                st.metric("üéØ Confidence", f"{confidence:.1%}")

            with col3:
                risk_level = "High" if prediction == 0 else "Low"
                st.metric("‚ö†Ô∏è Risk Level", risk_level)

            # Probability breakdown
            st.markdown("**üìä Detailed Probabilities:**")
            prob_data = {
                'Outcome': ['Dropout', 'Graduate', 'Enrolled'][:len(prediction_proba)],
                'Probability': prediction_proba
            }
            prob_df = pd.DataFrame(prob_data)

            fig = px.bar(prob_df, x='Outcome', y='Probability',
                         color='Probability', color_continuous_scale='RdYlGn_r',
                         title="Prediction Probabilities")
            st.plotly_chart(fig, use_container_width=True)

            # Generate explanation
            try:
                st.markdown("### üîç Why This Prediction?")

                with st.spinner("Generating explanation..."):
                    # SHAP explanation with better error handling
                    explainer = shap.TreeExplainer(model)

                    # FIXED: Ensure input is in the correct format for SHAP
                    # Convert DataFrame to numpy array if needed
                    input_array = input_df.values
                    st.write(f"Debug - Input array shape for SHAP: {input_array.shape}")

                    shap_values = explainer.shap_values(input_df)

                    # Handle different SHAP value formats
                    if isinstance(shap_values, list):
                        # Multi-class case
                        st.write(f"Debug - Multi-class SHAP values, using class {prediction}")
                        shap_values_single = shap_values[prediction]
                    elif shap_values.ndim == 3:
                        # 3D array format
                        st.write(f"Debug - 3D SHAP values, extracting class {prediction}")
                        shap_values_single = shap_values[0, :, prediction]
                    else:
                        # Binary classification or 2D array
                        st.write(f"Debug - 2D SHAP values")
                        shap_values_single = shap_values[0] if shap_values.ndim > 1 else shap_values

                    # Ensure shap_values_single is 1D
                    if hasattr(shap_values_single, 'ndim') and shap_values_single.ndim > 1:
                        shap_values_single = shap_values_single.flatten()

                    st.write(f"Debug - Final SHAP values shape: {shap_values_single.shape}")
                    st.write(f"Debug - Input values shape: {input_df.iloc[0].values.shape}")

                    # FIXED: Create feature contributions DataFrame more carefully
                    feature_contrib = pd.DataFrame({
                        'Feature': feature_list,
                        'Value': input_df.iloc[0].values.flatten(),  # Ensure 1D
                        'SHAP_Value': shap_values_single.flatten()  # Ensure 1D
                    })

                    feature_contrib['Abs_SHAP'] = np.abs(feature_contrib['SHAP_Value'])
                    feature_contrib = feature_contrib.sort_values('Abs_SHAP', ascending=False)

                    # Display top contributing features
                    st.markdown("**üéØ Top Factors Influencing This Prediction:**")

                    for i, (idx, row) in enumerate(feature_contrib.head(5).iterrows()):
                        impact = "increases" if row['SHAP_Value'] > 0 else "decreases"
                        color = "üî¥" if row['SHAP_Value'] > 0 else "üü¢"

                        st.write(f"{i + 1}. {color} **{row['Feature']}** (value: {row['Value']:.2f}) "
                                 f"{impact} dropout risk by {abs(row['SHAP_Value']):.4f}")

                    # Visualization of feature contributions
                    top_features = feature_contrib.head(10)
                    fig = px.bar(top_features, x='SHAP_Value', y='Feature',
                                 orientation='h', color='SHAP_Value',
                                 color_continuous_scale='RdBu_r',
                                 title="Feature Contributions to Prediction")
                    fig.update_layout(yaxis={'categoryorder': 'total ascending'})
                    st.plotly_chart(fig, use_container_width=True)

                    # Recommendations
                    st.markdown("### üí° Recommendations")
                    if prediction == 0:  # Dropout prediction
                        st.warning("**High Dropout Risk Detected!**")
                        st.markdown("**Recommended Interventions:**")

                        # Find features that increase dropout risk
                        risk_factors = feature_contrib[feature_contrib['SHAP_Value'] > 0].head(3)
                        for idx, row in risk_factors.iterrows():
                            st.write(f"‚Ä¢ Address **{row['Feature']}** - current value: {row['Value']:.2f}")

                        st.markdown("**Support Strategies:**")
                        st.write("‚Ä¢ Academic counseling and tutoring")
                        st.write("‚Ä¢ Financial aid consultation")
                        st.write("‚Ä¢ Student engagement programs")
                        st.write("‚Ä¢ Regular progress monitoring")

                    else:  # Graduate/Enrolled prediction
                        st.success("**Low Dropout Risk - Student on Track!**")
                        st.markdown("**Maintain Success Factors:**")

                        protective_factors = feature_contrib[feature_contrib['SHAP_Value'] < 0].head(3)
                        for idx, row in protective_factors.iterrows():
                            st.write(f"‚Ä¢ Continue supporting **{row['Feature']}** - current value: {row['Value']:.2f}")

            except Exception as e:
                st.error(f"Could not generate explanation: {str(e)}")
                st.info("Prediction completed, but explanation feature is unavailable.")

                # Additional debugging information
                st.write("**Debug Information:**")
                st.write(f"- Input DataFrame shape: {input_df.shape}")
                st.write(f"- Input DataFrame dtypes: {input_df.dtypes.tolist()}")
                st.write(f"- Prediction: {prediction}")
                st.write(f"- Prediction probabilities: {prediction_proba}")
                st.write(f"- Feature names length: {len(feature_list)}")
                st.write(f"- Model type: {type(model).__name__}")

                # Check for any array-like values in input_data
                st.write("**Input Data Analysis:**")
                for key, value in input_data.items():
                    st.write(f"- {key}: {type(value)} = {value}")

        except Exception as e:
            st.error(f"Error in prediction process: {str(e)}")
            st.write("**Debug Information:**")
            st.write(f"- Input data keys: {list(input_data.keys())}")
            st.write(f"- Expected features: {feature_list}")
            st.write(f"- Input data types: {[type(v) for v in input_data.values()]}")

            # Try to identify problematic values
            for key, value in input_data.items():
                if hasattr(value, 'shape'):
                    st.write(f"- {key} has shape: {value.shape} (this might be the problem)")
                elif hasattr(value, '__len__') and not isinstance(value, str):
                    st.write(f"- {key} has length: {len(value)} (this might be the problem)")

def main():
    st.markdown("<div style='font-size: 24px; font-weight: bold;'>üéì Student Dropout Prediction Dashboard</div>",
                unsafe_allow_html=True)
    st.markdown("""
    This interactive dashboard helps predict and analyze factors contributing to student dropout. 
    Navigate through the sections below to explore data, analyze trends, train models, and predict outcomes.
    """)
    if 'model_trained' not in st.session_state:
        st.session_state.model_trained = False
    if 'show_prediction' not in st.session_state:
        st.session_state.show_prediction = False

    st.sidebar.markdown("<div style='font-size: 20px; font-weight: bold;'>Navigation</div>", unsafe_allow_html=True)
    menu = ["Data Overview", "Exploratory Data Analysis", "Model Training & Evaluation", "Dropout Prediction"]
    choice = st.sidebar.radio("Select Module", menu, label_visibility="collapsed")

    st.sidebar.markdown("<div style='font-size: 16px;'>Data Input</div>", unsafe_allow_html=True)
    uploaded_file = st.sidebar.file_uploader("Upload your Student Dropout CSV", type=['csv'])
    df = load_data(uploaded_file)
    X, y, processed_df = preprocess_data(df)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    st.sidebar.markdown("---")
    if st.sidebar.button("Back to Top"):
        st.markdown("<script>window.scrollTo(0, 0);</script>", unsafe_allow_html=True)

    if choice == "Data Overview":
        st.markdown("<div style='font-size: 20px; font-weight: bold;'>üìä Data Overview</div>", unsafe_allow_html=True)
        st.markdown("Explore the dataset structure and key statistics.")

        sub_menu = st.selectbox("Select Analysis", [
            "Data Quality Assessment",
            "Demographics Insights",
            "Academic Performance",
            "Performance Summary",
            "Risk Factors Analysis",
            "Data Structure",
            "Feature Explorer",
            "Column Information"
        ], key="data_overview_submenu")

        with st.expander(f"{sub_menu}", expanded=True):
            if sub_menu == "Data Quality Assessment":
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("Total Students", f"{df.shape[0]:,}")
                with col2:
                    dropout_rate = (df['Target'] == 'Dropout').sum() / len(df) * 100
                    st.metric("Dropout Rate", f"{dropout_rate:.1f}%")
                with col3:
                    graduate_rate = (df['Target'] == 'Graduate').sum() / len(df) * 100
                    st.metric("Graduate Rate", f"{graduate_rate:.1f}%")
                with col4:
                    missing_data_pct = df.isnull().sum().sum() / (df.shape[0] * df.shape[1]) * 100
                    st.metric("Missing Data", f"{missing_data_pct:.1f}%")
                display_data_quality(df)
            elif sub_menu == "Demographics Insights":
                display_demographics(df)
            elif sub_menu == "Academic Performance":
                display_academic_performance(df)
            elif sub_menu == "Performance Summary":
                display_performance_summary(df)
            elif sub_menu == "Risk Factors Analysis":
                display_risk_factors(df)
            elif sub_menu == "Data Structure":
                display_data_structure(df)
            elif sub_menu == "Feature Explorer":
                display_feature_explorer(df)
            elif sub_menu == "Column Information":
                display_column_info(df)

    elif choice == "Exploratory Data Analysis":
        st.markdown("<div style='font-size: 20px; font-weight: bold;'>üìà Exploratory Data Analysis</div>",
                    unsafe_allow_html=True)
        st.markdown("Analyze data distributions, correlations, and relationships with student outcomes.")

        sub_menu = st.selectbox("Select Analysis", [
            "Distribution Analysis",
            "Correlation Analysis",
            "Outcome Analysis",
            "Academic Performance"
        ], key="eda_submenu")

        df_viz = df.copy()
        label_mappings = {
            'Gender': {0: 'Female', 1: 'Male'},
            'Scholarship holder': {0: 'No Scholarship', 1: 'Has Scholarship'},
            'Displaced': {0: 'Not Displaced', 1: 'Displaced'},
            'International': {0: 'Domestic', 1: 'International'},
            'Debtor': {0: 'No Debt', 1: 'Has Debt'},
            'Tuition fees up to date': {0: 'Fees Not Updated', 1: 'Fees Updated'},
            'Educational special needs': {0: 'No Special Needs', 1: 'Has Special Needs'},
            'Daytime/evening attendance': {0: 'Evening', 1: 'Daytime'}
        }

        for col, mapping in label_mappings.items():
            if col in df_viz.columns:
                df_viz[col] = df_viz[col].map(mapping).fillna(df_viz[col])

        with st.expander(f"{sub_menu}", expanded=True):
            if sub_menu == "Distribution Analysis":
                numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
                numeric_cols = [col for col in numeric_cols if col not in ['Target', 'id']]

                selected_columns = st.multiselect(
                    "Select features to visualize (up to 4)",
                    options=numeric_cols,
                    default=['Age at enrollment', 'Admission grade', 'Curricular units 1st sem (grade)'][:3],
                    max_selections=4
                )

                if selected_columns:
                    cols = st.columns(min(len(selected_columns), 2))
                    for i, column in enumerate(selected_columns):
                        with cols[i % len(cols)]:
                            fig, ax = plt.subplots(figsize=(8, 5))
                            sns.histplot(df[column], kde=True, ax=ax, color='skyblue', alpha=0.7)
                            ax.set_title(f'Distribution of {column}', fontsize=14, fontweight='bold', pad=15)
                            ax.set_xlabel(column, fontsize=12)
                            ax.set_ylabel('Number of Students', fontsize=12)
                            ax.tick_params(axis='both', labelsize=10)
                            plt.tight_layout()
                            st.pyplot(fig)
                            st.markdown(
                                f"**Insight**: The distribution of {column} shows its range and central tendency, impacting dropout risk.",
                                unsafe_allow_html=True)
                            plt.close()
                else:
                    st.info("Select at least one column to visualize.")

            elif sub_menu == "Correlation Analysis":
                numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
                correlation_features = st.multiselect(
                    "Select features for correlation analysis",
                    options=numeric_cols,
                    default=['Age at enrollment', 'Admission grade', 'Curricular units 1st sem (grade)',
                             'Curricular units 2nd sem (grade)'][:4]
                )

                if len(correlation_features) > 1:
                    col1, col2 = st.columns([2, 1])
                    with col1:
                        correlation_matrix = df[correlation_features].corr()
                        fig, ax = plt.subplots(figsize=(8, 6))
                        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', ax=ax, center=0, fmt='.2f',
                                    annot_kws={'size': 10})
                        ax.set_title('Correlation Matrix of Selected Features', fontsize=14, fontweight='bold', pad=15)
                        ax.tick_params(axis='both', labelsize=10)
                        plt.tight_layout()
                        st.pyplot(fig)
                        st.markdown(
                            "**Insight**: Strong correlations indicate features that move together, influencing dropout prediction.",
                            unsafe_allow_html=True)
                        plt.close()

                    with col2:
                        st.markdown("<h3 style='font-size: 16px;'>Correlation Insights</h3>", unsafe_allow_html=True)
                        corr_pairs = []
                        for i in range(len(correlation_matrix.columns)):
                            for j in range(i + 1, len(correlation_matrix.columns)):
                                corr_value = correlation_matrix.iloc[i, j]
                                if abs(corr_value) > 0.5:
                                    corr_pairs.append(
                                        (correlation_matrix.columns[i], correlation_matrix.columns[j], corr_value))

                        if corr_pairs:
                            corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)
                            for feat1, feat2, corr in corr_pairs[:5]:
                                direction = "positive" if corr > 0 else "negative"
                                st.write(f"‚Ä¢ **{feat1}** & **{feat2}**: {direction} ({corr:.2f})")
                            st.markdown(
                                "**Insight**: High correlations highlight key feature relationships affecting student outcomes.",
                                unsafe_allow_html=True)
                        else:
                            st.write("No strong correlations (>0.5) found.")
                            st.markdown(
                                "**Insight**: Weak correlations suggest features are independent, requiring diverse predictors.",
                                unsafe_allow_html=True)
                else:
                    st.info("Select at least two features for correlation analysis.")

            elif sub_menu == "Outcome Analysis":
                col1, col2 = st.columns([1, 1])
                with col1:
                    fig, ax = plt.subplots(figsize=(8, 5))
                    target_counts = df['Target'].value_counts()
                    ax.pie(target_counts.values, labels=target_counts.index, autopct='%1.1f%%',
                           colors=['#ff9999', '#66b3ff', '#99ff99'], textprops={'fontsize': 10})
                    ax.set_title('Distribution of Student Outcomes', fontsize=14, fontweight='bold', pad=15)
                    plt.tight_layout()
                    st.pyplot(fig)
                    st.markdown(
                        "**Insight**: The proportion of Graduates, Dropouts, and Enrolled students shows the dataset's outcome balance.",
                        unsafe_allow_html=True)
                    plt.close()

                with col2:
                    st.markdown("<h3 style='font-size: 16px;'>Outcome Statistics</h3>", unsafe_allow_html=True)
                    total_students = len(df)
                    for outcome in df['Target'].unique():
                        count = (df['Target'] == outcome).sum()
                        percentage = count / total_students * 100
                        st.write(f"‚Ä¢ **{outcome}**: {count:,} students ({percentage:.1f}%)")
                    st.markdown(
                        "**Insight**: Dropout rates indicate the scale of the challenge in improving student retention.",
                        unsafe_allow_html=True)

                factor_cols = st.multiselect(
                    "Select factors to analyze against outcomes",
                    options=['Gender', 'Scholarship holder', 'Displaced', 'International'],
                    default=['Gender', 'Scholarship holder']
                )

                if factor_cols:
                    cols = st.columns(min(len(factor_cols), 2))
                    for i, col in enumerate(factor_cols):
                        with cols[i % len(cols)]:
                            fig, ax = plt.subplots(figsize=(8, 5))
                            cross_tab = pd.crosstab(df_viz[col], df_viz['Target'])
                            cross_tab_pct = cross_tab.div(cross_tab.sum(axis=1), axis=0) * 100
                            cross_tab_pct.plot(kind='bar', ax=ax, color=['#ff9999', '#66b3ff', '#99ff99'])
                            ax.set_title(f'Outcome Distribution by {col}', fontsize=14, fontweight='bold', pad=15)
                            ax.set_xlabel(col, fontsize=12)
                            ax.set_ylabel('Percentage of Students', fontsize=12)
                            ax.tick_params(axis='both', labelsize=10)
                            ax.legend(title='Outcome', fontsize=10)
                            plt.tight_layout()
                            st.pyplot(fig)
                            st.markdown(
                                f"**Insight**: {col} influences student outcomes, with certain categories linked to higher dropout rates.",
                                unsafe_allow_html=True)
                            plt.close()

            elif sub_menu == "Academic Performance":
                academic_cols = [col for col in df.columns if 'grade' in col.lower() or 'approved' in col.lower()]
                selected_academic = st.multiselect(
                    "Select academic performance metrics",
                    options=academic_cols,
                    default=['Curricular units 1st sem (grade)', 'Curricular units 1st sem (approved)'][:2]
                )

                if selected_academic:
                    for col in selected_academic:
                        friendly_name = col.replace('Curricular units 1st sem (grade)', 'First Semester Grades') \
                            .replace('Curricular units 1st sem (approved)', 'First Semester Units Passed') \
                            .replace('Curricular units 2nd sem (grade)', 'Second Semester Grades') \
                            .replace('Curricular units 2nd sem (approved)', 'Second Semester Units Passed')

                        st.markdown(f"<h3 style='font-size: 16px;'>{friendly_name} Analysis</h3>",
                                    unsafe_allow_html=True)
                        col1, col2 = st.columns([2, 1])

                        with col1:
                            fig, ax = plt.subplots(figsize=(10, 5))
                            colors = ['lightcoral', 'lightgreen', 'lightskyblue']
                            box_plot = ax.boxplot(
                                [df[df['Target'] == outcome][col].dropna() for outcome in df['Target'].unique()],
                                labels=df['Target'].unique(),
                                patch_artist=True)

                            for patch, color in zip(box_plot['boxes'], colors):
                                patch.set_facecolor(color)
                                patch.set_alpha(0.7)

                            ax.set_title(f'{friendly_name} by Student Outcome', fontsize=14, fontweight='bold', pad=15)
                            ax.set_xlabel('Student Outcome', fontsize=12)
                            ax.set_ylabel(friendly_name, fontsize=12)
                            ax.tick_params(axis='both', labelsize=10)
                            plt.tight_layout()
                            st.pyplot(fig)
                            st.markdown(
                                f"**Insight**: Higher {friendly_name.lower()} are associated with graduates, while lower values correlate with dropouts.",
                                unsafe_allow_html=True)
                            plt.close()

                        with col2:
                            st.markdown("<h3 style='font-size: 16px;'>Quick Summary</h3>", unsafe_allow_html=True)
                            summary_stats = df.groupby('Target')[col].agg(['mean', 'count']).round(2)
                            for outcome in summary_stats.index:
                                mean_val = summary_stats.loc[outcome, 'mean']
                                count_val = int(summary_stats.loc[outcome, 'count'])
                                emoji = "üéì" if outcome == 'Graduate' else "‚ö†Ô∏è" if outcome == 'Dropout' else "üìö"
                                st.write(f"{emoji} **{outcome}:** Average: {mean_val}, Students: {count_val}")
                            st.markdown(
                                f"**Insight**: {friendly_name} significantly differentiates outcomes, guiding intervention strategies.",
                                unsafe_allow_html=True)

    elif choice == "Model Training & Evaluation":
        st.markdown("<div style='font-size: 20px; font-weight: bold;'>ü§ñ Model Training & Evaluation</div>",
                    unsafe_allow_html=True)
        st.markdown("Train a machine learning model and evaluate its performance.")

        sub_menu = st.selectbox("Select Action", [
            "Train Model",
            "View Results",
            "Model Explainability"  # NEW: Added explainability option
        ], key="model_submenu")

        if sub_menu == "Train Model":
            with st.expander("Model Training", expanded=True):
                target_mapping_df = pd.DataFrame({
                    'Original': list(TARGET_MAPPING.keys()),
                    'Encoded': list(TARGET_MAPPING.values())
                })
                st.markdown("<h3 style='font-size: 16px;'>Target Label Encoding</h3>", unsafe_allow_html=True)
                st.dataframe(target_mapping_df)
                st.write("Target values found:", processed_df['Target'].unique().tolist())
                st.markdown(
                    "**Insight**: Encoded labels ensure the model can process categorical outcomes effectively.",
                    unsafe_allow_html=True)

                if st.button("Start Training"):
                    with st.spinner("Training model..."):
                        model = train_model(X_train, y_train)
                        st.session_state.model = model
                        st.session_state.model_trained = True
                        # Store training data for explainability
                        st.session_state.X_train = X_train
                        st.session_state.X_test = X_test
                        st.session_state.y_test = y_test
                        st.session_state.feature_names = X.columns.tolist()
                        st.success("Model trained successfully!")
                        st.button("View Results", key="go_to_results")

        elif sub_menu == "View Results":
            if st.session_state.model_trained:
                with st.expander("Model Results", expanded=True):
                    visualize_model_results(st.session_state.model, X_test, y_test)
                    if st.button("Proceed to Dropout Prediction"):
                        st.session_state.show_prediction = True
                        st.experimental_rerun()
            else:
                st.warning("Please train the model first!")

        # NEW: Model Explainability Section
        elif sub_menu == "Model Explainability":
            if st.session_state.model_trained and 'model' in st.session_state:
                with st.expander("Model Explainability Dashboard", expanded=True):
                    st.markdown("### üîç Understanding Model Decisions")
                    st.markdown("Explore how the model makes predictions and which features are most important.")

                    explainability_type = st.selectbox(
                        "Select Explainability Analysis",
                        ["Global Feature Importance", "Local Prediction Explanation", "Feature Impact Analysis"]
                    )

                    if explainability_type == "Global Feature Importance":
                        display_global_feature_importance(
                            st.session_state.model,
                            st.session_state.feature_names
                        )

                    elif explainability_type == "Local Prediction Explanation":
                        display_local_explanation(
                            st.session_state.model,
                            st.session_state.X_train,
                            st.session_state.X_test,
                            st.session_state.feature_names
                        )

                    elif explainability_type == "Feature Impact Analysis":
                        display_feature_impact_analysis(
                            st.session_state.model,
                            st.session_state.X_train,
                            st.session_state.X_test,
                            st.session_state.feature_names,
                            df
                        )
            else:
                st.warning("Please train the model first to access explainability features!")
                if st.button("Go to Model Training"):
                    st.experimental_rerun()

    elif choice == "Dropout Prediction" or st.session_state.show_prediction:
        st.markdown("<div style='font-size: 20px; font-weight: bold;'>üîÆ Dropout Prediction</div>",
                    unsafe_allow_html=True)
        st.markdown("Predict the likelihood of dropout for an individual student.")

        if 'model' in st.session_state:
            with st.expander("Prediction Tool", expanded=True):
                # Modified to include explainability in individual predictions
                individual_dropout_prediction_with_explanation(
                    st.session_state.model,
                    X,
                    st.session_state.X_train,
                    st.session_state.feature_names
                )
        else:
            st.warning("Please train the model first!")
            if st.button("Go to Model Training"):
                st.session_state.show_prediction = False
                st.experimental_rerun()


if __name__ == "__main__":
    main()
